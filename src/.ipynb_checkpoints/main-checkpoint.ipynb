{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import models\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.utils import dense_to_sparse, to_networkx\n",
    "import subgraph\n",
    "\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "importlib.reload(subgraph)\n",
    "\n",
    "### importing OGB\n",
    "from ogb.graphproppred import PygGraphPropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_info(batch):\n",
    "    # DataBatch(edge_index=[2, 1826], edge_attr=[1826, 3], x=[847, 9], y=[32, 1], num_nodes=847, batch=[847], ptr=[33])\n",
    "    n_nodes = batch.x.shape[0]\n",
    "    n_edges = batch.edge_index.shape[1]\n",
    "    nhbr_info = subgraph.compute_nhbr_pair_data(to_networkx(batch), batch.edge_index, edge_feat, edge_only)\n",
    "    adj = torch.sparse_coo_tensor(batch.edge_index, torch.ones(n_edges), (n_nodes, n_nodes)).coalesce()\n",
    "    return nhbr_info, adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_criterion = torch.nn.BCEWithLogitsLoss()\n",
    "reg_criterion = torch.nn.MSELoss()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "train_nhbr_map = {}\n",
    "val_nhbr_map = {}\n",
    "test_nhbr_map = {}\n",
    "\n",
    "def train(model, device, loader, optimizer, task_type):\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        # batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1 or batch.batch[-1] == 0:\n",
    "            pass\n",
    "        else:\n",
    "            # if step not in train_nhbr_map:\n",
    "            #     train_nhbr_map[batch.edge_index] = get_pair_info(batch)\n",
    "            # nhbr_info, adj = train_nhbr_map[batch.edge_index]\n",
    "            nhbr_info, adj = get_pair_info(batch)\n",
    "            con, con_sct, not_con, not_con_sct = nhbr_info\n",
    "            con = con.to(device)\n",
    "            con_sct = con_sct.to(device)\n",
    "            not_con = not_con.to(device)\n",
    "            not_con_sct = not_con_sct.to(device)\n",
    "            adj = adj.to(device)\n",
    "            x = batch.x.to(device)\n",
    "            e = torch.vstack((batch.edge_attr, torch.zeros((1, 3)))).to(device)\n",
    "            batch_idx = batch.batch.to(device)\n",
    "            pred = model(x, e, adj, (con, con_sct, not_con, not_con_sct), batch_idx)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            ## ignore nan targets (unlabeled) when computing training loss.\n",
    "            is_labeled = batch.y == batch.y\n",
    "            if \"classification\" in task_type: \n",
    "                loss = cls_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(device).to(torch.float32)[is_labeled])\n",
    "            else:\n",
    "                loss = reg_criterion(pred.to(torch.float32)[is_labeled], batch.y.to(device).to(torch.float32)[is_labeled])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def eval(model, device, loader, evaluator, split_type):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    if split_type == \"train\":\n",
    "        mapp = train_nhbr_map\n",
    "    elif split_type == \"val\":\n",
    "        mapp = val_nhbr_map\n",
    "    else:\n",
    "        mapp = test_nhbr_map\n",
    "\n",
    "    for step, batch in enumerate(tqdm(loader, desc=\"Iteration\")):\n",
    "        # batch = batch.to(device)\n",
    "\n",
    "        if batch.x.shape[0] == 1:\n",
    "            pass\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                # if step not in mapp:\n",
    "                #     mapp[batch.edge_index] = get_pair_info(batch)\n",
    "                # nhbr_info, adj = mapp[batch.edge_index]\n",
    "                nhbr_info, adj = get_pair_info(batch)\n",
    "                con, con_sct, not_con, not_con_sct = nhbr_info\n",
    "                con = con.to(device)\n",
    "                con_sct = con_sct.to(device)\n",
    "                not_con = not_con.to(device)\n",
    "                not_con_sct = not_con_sct.to(device)\n",
    "                adj = adj.to(device)\n",
    "                x = batch.x.to(device)\n",
    "                e = torch.vstack((batch.edge_attr, torch.zeros((1, 3)))).to(device)\n",
    "                batch_idx = batch.batch.to(device)\n",
    "                pred = model(x, e, adj, (con, con_sct, not_con, not_con_sct), batch_idx)\n",
    "\n",
    "            y_true.append(batch.y.view(pred.shape).detach().cpu())\n",
    "            y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Epoch 1\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:   6%|███                                               | 62/1029 [00:02<00:32, 29.53it/s]"
     ]
    }
   ],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='GNN baselines on ogbgmol* data with Pytorch Geometrics')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--gnn', type=str, default='gin-virtual',\n",
    "                    help='GNN gin, gin-virtual, or gcn, or gcn-virtual (default: gin-virtual)')\n",
    "parser.add_argument('--drop_ratio', type=float, default=0.5,\n",
    "                    help='dropout ratio (default: 0.5)')\n",
    "parser.add_argument('--num_layer', type=int, default=5,\n",
    "                    help='number of GNN message passing layers (default: 5)')\n",
    "parser.add_argument('--emb_dim', type=int, default=300,\n",
    "                    help='dimensionality of hidden units in GNNs (default: 300)')\n",
    "parser.add_argument('--batch_size', type=int, default=32,\n",
    "                    help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--epochs', type=int, default=100,\n",
    "                    help='number of epochs to train (default: 100)')\n",
    "parser.add_argument('--num_workers', type=int, default=0,\n",
    "                    help='number of workers (default: 0)')\n",
    "parser.add_argument('--dataset', type=str, default=\"ogbg-molhiv\",\n",
    "                    help='dataset name (default: ogbg-molhiv)')\n",
    "parser.add_argument('--lr', default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--edge_feat', action='store_true', default=False, \n",
    "                    help='Inject edge features')\n",
    "parser.add_argument('--edge_only', action='store_true', default=False, \n",
    "                    help='Aggregate edge only pairs in neighbourhood.')\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "edge_feat = args.edge_feat\n",
    "edge_only = args.edge_only\n",
    "\n",
    "### automatic dataloading and splitting\n",
    "dataset = PygGraphPropPredDataset(name = args.dataset)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n",
    "\n",
    "### automatic evaluator. takes dataset name as input\n",
    "evaluator = Evaluator(args.dataset)\n",
    "\n",
    "train_loader = DataLoader(dataset[split_idx[\"train\"]], batch_size=args.batch_size, shuffle=True, num_workers = args.num_workers)\n",
    "valid_loader = DataLoader(dataset[split_idx[\"valid\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "test_loader = DataLoader(dataset[split_idx[\"test\"]], batch_size=args.batch_size, shuffle=False, num_workers = args.num_workers)\n",
    "\n",
    "model = models.GNN_graphprop(nfeat=dataset.data.x.shape[1] + args.emb_dim, \n",
    "                        nhid=args.emb_dim, \n",
    "                        nclass=1,  # BCE for molhiv\n",
    "                        nlayers=args.num_layer,\n",
    "                        dropout=args.drop_ratio,\n",
    "                        edge_feat=edge_feat)  # TODO consider virtual node\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "valid_curve = []\n",
    "test_curve = []\n",
    "train_curve = []\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(\"=====Epoch {}\".format(epoch))\n",
    "    print('Training...')\n",
    "    train(model, device, train_loader, optimizer, dataset.task_type)\n",
    "\n",
    "    print('Evaluating...')\n",
    "    train_perf = eval(model, device, train_loader, evaluator, \"train\")\n",
    "    valid_perf = eval(model, device, valid_loader, evaluator, \"val\")\n",
    "    test_perf = eval(model, device, test_loader, evaluator, \"test\")\n",
    "    print({'Train': train_perf, 'Validation': valid_perf, 'Test': test_perf})\n",
    "    train_curve.append(train_perf[dataset.eval_metric])\n",
    "    valid_curve.append(valid_perf[dataset.eval_metric])\n",
    "    test_curve.append(test_perf[dataset.eval_metric])\n",
    "\n",
    "if 'classification' in dataset.task_type:\n",
    "    best_val_epoch = np.argmax(np.array(valid_curve))\n",
    "    best_train = max(train_curve)\n",
    "else:\n",
    "    best_val_epoch = np.argmin(np.array(valid_curve))\n",
    "    best_train = min(train_curve)\n",
    "\n",
    "print('Finished training!')\n",
    "print('Best validation score: {}'.format(valid_curve[best_val_epoch]))\n",
    "print('Test score: {}'.format(test_curve[best_val_epoch]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0053a58cc95ba184e35d177a61c645ff822d8681cabe10dabb27d8d22f7ed4f0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
