{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc08ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.5.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.optim as optim\n",
    "import models as models\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import time\n",
    "import numpy as np\n",
    "import util\n",
    "import subgraph\n",
    "from torch_geometric.datasets import ZINC\n",
    "from train.zinc import train, eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"ZINC\")\n",
    "parser.add_argument('--d', type=int, default=3,\n",
    "                    help='distance of neighbourhood (default: 1)')\n",
    "parser.add_argument('--t', type=int, default=2,\n",
    "                    help='size of t-subsets (default: 2)')\n",
    "parser.add_argument('--scalar', type=bool, default=True,\n",
    "                    help='learn scalars')\n",
    "parser.add_argument('--no-connected', dest='connected', action='store_false',\n",
    "                    help='also consider disconnected t-subsets')\n",
    "\n",
    "parser.add_argument('--drop_ratio', type=float, default=0.0,\n",
    "                    help='dropout ratio')\n",
    "parser.add_argument('--num_layer', type=int, default=4,\n",
    "                    help='number of GNN message passing layers')\n",
    "parser.add_argument('--emb_dim', type=int, default=80,\n",
    "                    help='dimensionality of hidden units in GNNs')\n",
    "parser.add_argument('--readout', type=str, default=\"sum\", choices=[\"sum\", \"mean\"],\n",
    "                    help='readout')\n",
    "parser.add_argument('--combination', type=str, default=\"multi\", choices=[\"sum\", \"multi\"],\n",
    "                    help='pair combination operation')\n",
    "parser.add_argument('--mlp', type=bool, default=False,\n",
    "                    help=\"mlp (default: False)\")\n",
    "parser.add_argument('--jk', type=bool, default=False,\n",
    "                    help=\"jk\")\n",
    "parser.add_argument('--multiplier', type=int, default=1,\n",
    "                    help=\"hidden layer readout multiplier\")\n",
    "\n",
    "parser.add_argument('--edge_features', dest='edge_features', action='store_false',\n",
    "                    help='exist edge attributes')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=128,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--step', type=int, default=20,\n",
    "                    help='input batch size for training')\n",
    "parser.add_argument('--epochs', type=int, default=1000,\n",
    "                    help='number of epochs to train (default: 1000)')\n",
    "parser.add_argument('--device', type=int, default=0,\n",
    "                    help='which gpu to use if any (default: 0)')\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97853f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.device)  \n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available with GPU:',torch.cuda.get_device_name(0))\n",
    "device = torch.device(f\"cuda:{args.device}\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8770a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = ZINC(root=\"dataset/ZINC\", split='train', subset=True)  # subset loads 12k instead of 250k\n",
    "valset = ZINC(root=\"dataset/ZINC\", split='val', subset=True)\n",
    "testset = ZINC(root=\"dataset/ZINC\", split='test', subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb321fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offline preprocessing\n",
    "pathlib.Path('preprocessed').mkdir(parents=True, exist_ok=True) \n",
    "save_file = f'preprocessed/zinc_{args.d}_{args.t}_{args.connected}.data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82706e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pair infomation...\n",
      "Computing pair infomation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:33<00:00,  2.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.55it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8/8 [00:03<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair infomation computed! Time: 39.742677450180054\n",
      "Saving pair infomation...\n",
      "Pair infomation saved! Time: 3.998828172683716\n",
      "topology types: {(0, 1), (1, 2), (1, 1), (2, 3), (3, 3), (2, 2)}\n",
      "number of topology types: 6\n",
      "emb_dim: 80\n",
      "number of parameters: 208265\n",
      "Warning: 100000 parameter budget exceeded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169:  17%|██████████████████████████▎                                                                                                                                 | 169/1000 [6:21:57<33:16:51, 144.18s/it, lr=0.001, test_loss=0.154, train_loss=0.124, val_loss=0.178]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00169: reducing learning rate of group 0 to 5.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225:  22%|██████████████████████████████████▋                                                                                                                       | 225/1000 [8:30:08<29:16:16, 135.97s/it, lr=0.0005, test_loss=0.134, train_loss=0.0703, val_loss=0.165]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00225: reducing learning rate of group 0 to 2.5000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 275:  28%|█████████████████████████████████████████▊                                                                                                              | 275/1000 [10:23:05<27:30:33, 136.60s/it, lr=0.00025, test_loss=0.126, train_loss=0.0451, val_loss=0.157]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00275: reducing learning rate of group 0 to 1.2500e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 296:  30%|████████████████████████████████████████████▋                                                                                                          | 296/1000 [11:11:56<26:33:25, 135.80s/it, lr=0.000125, test_loss=0.129, train_loss=0.0348, val_loss=0.156]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00296: reducing learning rate of group 0 to 6.2500e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 317:  32%|████████████████████████████████████████████████▊                                                                                                         | 317/1000 [12:00:13<27:02:20, 142.52s/it, lr=6.25e-5, test_loss=0.125, train_loss=0.03, val_loss=0.152]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00317: reducing learning rate of group 0 to 3.1250e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 338:  34%|███████████████████████████████████████████████████▋                                                                                                     | 338/1000 [12:48:42<24:41:50, 134.31s/it, lr=3.13e-5, test_loss=0.125, train_loss=0.027, val_loss=0.153]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00338: reducing learning rate of group 0 to 1.5625e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 373:  37%|████████████████████████████████████████████████████████▋                                                                                               | 373/1000 [14:10:10<23:49:06, 136.76s/it, lr=1.56e-5, test_loss=0.125, train_loss=0.0256, val_loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00374: reducing learning rate of group 0 to 7.8125e-06.\n",
      "\n",
      "!! LR EQUAL TO MIN LR SET.\n",
      "Test MAE: 0.1247\n",
      "Train MAE: 0.0600\n",
      "Convergence Time (Epochs): 373.0000\n",
      "TOTAL TIME TAKEN: 51120.0192s\n",
      "AVG TIME PER EPOCH: 136.3893s\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print('Loading pair infomation...')\n",
    "    time_t = time.time()\n",
    "    train_loader, valid_loader, test_loader = torch.load(save_file)\n",
    "    print('Pair infomation loaded! Time:', time.time() - time_t)\n",
    "except:\n",
    "    print('Computing pair infomation...')\n",
    "    time_t = time.time()\n",
    "    train_loader = []\n",
    "    valid_loader = []\n",
    "    test_loader = []\n",
    "    for batch in tqdm(DataLoader(trainset, batch_size=args.batch_size, shuffle=True)):\n",
    "        train_loader.append(subgraph.transform(batch, args.d, args.t, args.connected))\n",
    "    train_loader = DataLoader(train_loader, batch_size=1, shuffle=True)\n",
    "    for batch in tqdm(DataLoader(valset, batch_size=args.batch_size, shuffle=False)):\n",
    "        valid_loader.append(subgraph.transform(batch, args.d, args.t, args.connected))\n",
    "    valid_loader = DataLoader(valid_loader, batch_size=1, shuffle=False)\n",
    "    for batch in tqdm(DataLoader(testset, batch_size=args.batch_size, shuffle=False)):\n",
    "        test_loader.append(subgraph.transform(batch, args.d, args.t, args.connected))\n",
    "    test_loader = DataLoader(test_loader, batch_size=1, shuffle=False)\n",
    "    print('Pair infomation computed! Time:', time.time() - time_t)\n",
    "    print('Saving pair infomation...')\n",
    "    time_t = time.time()\n",
    "    torch.save((train_loader, valid_loader, test_loader), save_file)\n",
    "    print('Pair infomation saved! Time:', time.time() - time_t)\n",
    "\n",
    "params = {\n",
    "    'nfeat':28, #num of atom type\n",
    "    'edge_attr': 4, #num of bond type\n",
    "    'exist_edge_attr': args.edge_features,\n",
    "    'nhid':args.emb_dim, \n",
    "    'nclass':1,   # 1 out dim since regression problem \n",
    "    'nlayers':args.num_layer,\n",
    "    'dropout':args.drop_ratio,\n",
    "    'readout':args.readout,\n",
    "    'd':args.d,\n",
    "    't':args.t, \n",
    "    'scalar':args.scalar,  \n",
    "    'mlp':args.mlp, \n",
    "    'jk':args.jk, \n",
    "    'combination':args.combination,\n",
    "    'multiplier':args.multiplier,\n",
    "    'keys':subgraph.get_keys_from_loaders([train_loader, valid_loader, test_loader]),\n",
    "}\n",
    "\n",
    "model = models.GNN_bench(params).to(device)\n",
    "\n",
    "n_params = util.get_n_params(model)\n",
    "print('emb_dim:', args.emb_dim)\n",
    "print('number of parameters:', util.get_n_params(model))\n",
    "if n_params > 110000:\n",
    "    print(f'Warning: 100000 parameter budget exceeded.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
    "                                                    factor=0.5,\n",
    "                                                    patience=args.step,\n",
    "                                                    verbose=True)\n",
    "\n",
    "t0 = time.time()\n",
    "per_epoch_time = []\n",
    "epoch_train_losses, epoch_val_losses = [], []\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    with tqdm(range(args.epochs)) as tq:\n",
    "        for epoch in tq:\n",
    "\n",
    "            tq.set_description('Epoch %d' % epoch)\n",
    "\n",
    "            startime_t = time.time()\n",
    "\n",
    "            epoch_train_loss, optimizer = train(model, optimizer, train_loader, epoch, device)\n",
    "\n",
    "            epoch_val_loss = eval(model, valid_loader, epoch, device)\n",
    "            epoch_test_loss = eval(model, test_loader, epoch, device)                \n",
    "\n",
    "            epoch_train_losses.append(epoch_train_loss)\n",
    "            epoch_val_losses.append(epoch_val_loss)\n",
    "\n",
    "            tq.set_postfix(lr=optimizer.param_groups[0]['lr'],\n",
    "                          train_loss=epoch_train_loss, val_loss=epoch_val_loss, test_loss=epoch_test_loss)\n",
    "\n",
    "            per_epoch_time.append(time.time() - startime_t)\n",
    "\n",
    "            scheduler.step(epoch_val_loss)\n",
    "\n",
    "            if optimizer.param_groups[0]['lr'] < 1e-5:\n",
    "                print(\"\\n!! LR EQUAL TO MIN LR SET.\")\n",
    "                break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early because of KeyboardInterrupt')\n",
    "\n",
    "test_mae = eval(model, test_loader, epoch, device)\n",
    "train_mae = eval(model, train_loader, epoch, device)\n",
    "print(\"Test MAE: {:.4f}\".format(test_mae))\n",
    "print(\"Train MAE: {:.4f}\".format(train_mae))\n",
    "print(\"Convergence Time (Epochs): {:.4f}\".format(epoch))\n",
    "print(\"TOTAL TIME TAKEN: {:.4f}s\".format(time.time()-t0))\n",
    "print(\"AVG TIME PER EPOCH: {:.4f}s\".format(np.mean(per_epoch_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0c4a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
